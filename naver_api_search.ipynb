{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests in /Users/kimminsu/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (2.28.1)\n",
      "Requirement already satisfied: packaging in /Users/kimminsu/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/kimminsu/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kimminsu/opt/anaconda3/lib/python3.9/site-packages (from packaging->webdriver-manager) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kimminsu/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kimminsu/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kimminsu/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kimminsu/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (3.3)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 ~  30 페이지 까지 크롤링을 진행 합니다\n",
      "\n",
      "한번에 가져올 페이지 :  10 페이지\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# WebDriver Setting\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "service = Service(ChromeDriverManager().install())  # Service 객체 생성\n",
    "driver = webdriver.Chrome(service=service, options=options)  # Service와 옵션을 함께 전달\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "\n",
    "# selenium으로 검색 페이지 불러오기 #\n",
    "naver_urls = []\n",
    "pub_dates = []\n",
    "\n",
    "# Naver API key 입력\n",
    "client_id = '' \n",
    "client_secret = ''\n",
    "\n",
    "# 검색어 입력\n",
    "keword = input(\"검색할 키워드를 입력해주세요:\")\n",
    "encText = urllib.parse.quote(keword)\n",
    "\n",
    "# 검색을 끝낼 페이지 입력\n",
    "end = input(\"\\n크롤링을 끝낼 페이지 위치를 입력해주세요. (기본값:1, 최대값:100):\")  \n",
    "if end == \"\":\n",
    "    end = 1\n",
    "else:\n",
    "    end = int(end)\n",
    "print(\"\\n 1 ~ \", end, \"페이지 까지 크롤링을 진행 합니다\")\n",
    "\n",
    "# 한번에 가져올 페이지 입력\n",
    "display = input(\"\\n한번에 가져올 페이지 개수를 입력해주세요.(기본값:10, 최대값: 100):\")\n",
    "if display == \"\":\n",
    "    display = 10\n",
    "else:\n",
    "    display = int(display)\n",
    "print(\"\\n한번에 가져올 페이지 : \", display, \"페이지\")\n",
    "\n",
    "\n",
    "for start in range(end):\n",
    "    url = \"https://openapi.naver.com/v1/search/news?query=\" + encText + \"&start=\" + str(start+1) + \"&display=\" + str(display+1) # JSON 결과\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if(rescode==200):\n",
    "        response_body = response.read()\n",
    "        data = json.loads(response_body.decode('utf-8'))['items']\n",
    "        for row in data:\n",
    "            if('news.naver' in row['link']):\n",
    "                naver_urls.append(row['link'])\n",
    "                pub_dates.append(row['pubDate'])\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###naver 기사 본문 및 제목 가져오기###\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "comments_texts = []\n",
    "for i in naver_urls:\n",
    "    original_html = requests.get(i, headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 제목 가져오기\n",
    "title = html.select(\"div#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "# list합치기\n",
    "title = ''.join(str(title))\n",
    "# html태그제거\n",
    "pattern1 = '<[^>]*>'\n",
    "title = re.sub(pattern=pattern1, repl='', string=title)\n",
    "titles.append(title)\n",
    "\n",
    "# 뉴스 본문 가져오기\n",
    "\n",
    "content = html.select(\"div#dic_area\")\n",
    "\n",
    "# 기사 텍스트만 가져오기\n",
    "# list합치기\n",
    "content = ''.join(str(content))\n",
    "\n",
    "# html태그제거 및 텍스트 다듬기\n",
    "content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "content = content.replace(pattern2, '')\n",
    "contents.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 댓글 가져오기\n",
    "driver.get(i)\n",
    "time.sleep(1)  # 대기시간 변경 가능\n",
    "# 네이버 댓글 눌러서 댓글 가져오기#\n",
    "a = driver.find_element(By.CSS_SELECTOR, 'a._COMMENT_COUNT_VIEW')\n",
    "\n",
    "# 댓글 더보기 클릭\n",
    "a.click()\n",
    "\n",
    "# 대기시간 변경 가능\n",
    "time.sleep(3)  \n",
    "\n",
    "# 네이버 뉴스 댓글 가져오기\n",
    "html = driver.page_source\n",
    "c_soup = BeautifulSoup(html)\n",
    "\n",
    "comments = c_soup.select('span.u_cbox_contents')\n",
    "comments_text = ', '.join([comment.text.strip() for comment in comments])\n",
    "comments_texts.append(comments_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles: 1\n",
      "Links: 174\n",
      "Contents: 1\n",
      "Comments: 1\n",
      "Dates: 174\n"
     ]
    }
   ],
   "source": [
    "print(f\"Titles: {len(titles)}\")\n",
    "print(f\"Links: {len(naver_urls)}\")\n",
    "print(f\"Contents: {len(contents)}\")\n",
    "print(f\"Comments: {len(comments_texts)}\")\n",
    "print(f\"Dates: {len(pub_dates)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
